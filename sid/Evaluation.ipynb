{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import padertorch as pt\n",
    "import padercontrib as pc\n",
    "import paderbox as pb\n",
    "from padertorch import Model\n",
    "from sacred import Experiment, commands\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "from paderbox.io.new_subdir import get_new_subdir\n",
    "from paderbox.io import load_json, dump_json\n",
    "from pprint import pprint\n",
    "from fearless.sid.data import prepare_data\n",
    "from fearless.sid.data import prepare_data_2\n",
    "from scipy.special import softmax\n",
    "from scipy.spatial.distance import euclidean as euc\n",
    "from statistics import mean\n",
    "import tensorflow as tf\n",
    "import pb_sed.evaluation.instance_based as sed\n",
    "import lazy_dataset\n",
    "from padertorch.data.utils import collate_fn\n",
    "from padertorch.ops.sequence.pack_module import pad_sequence\n",
    "from padercontrib.database.fearless import Fearless\n",
    "import padertorch as pt\n",
    "import paderbox as pb\n",
    "import numpy as np\n",
    "import paderbox as pb\n",
    "from padercontrib.database.iterator import AudioReader\n",
    "from paderbox.transform import stft,fbank\n",
    "import scipy\n",
    "import pydub\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import pandas as pd\n",
    "from pydub import AudioSegment\n",
    "import torch\n",
    "from padertorch.data.utils import collate_fn\n",
    "import lazy_dataset\n",
    "from padertorch.ops.sequence.pack_module import pad_sequence\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fearless.sid.data import prepare_data_3\n",
    "from fearless.sid.data import prepare_data_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Fearless()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(example, batch_size, shuffle = False):\n",
    "    \n",
    "\n",
    "    ds = example\n",
    "    \n",
    "    def prep_features(example):\n",
    "        padded_audio = []\n",
    "        fbank_data = []\n",
    "        \n",
    "        \"\"\" Obtain audio segments from the dataset\"\"\"\n",
    "        \"\"\" If segments smaller than 4secs, pad with silence. Else, extract 4secs from larger audio segments \"\"\"\n",
    "        audio_sam = AudioSegment.from_wav(example['audio_path']['observation'])\n",
    "        if audio_sam.duration_seconds < 4:\n",
    "            pad_ms = (4 - audio_sam.duration_seconds)*1000\n",
    "            silence = AudioSegment.silent(duration=pad_ms) # milliseconds of silence needed\n",
    "            padded = audio_sam + silence\n",
    "            \n",
    "        elif audio_sam.duration_seconds >= 4:\n",
    "            pad_ms = 0\n",
    "            audio_sam = audio_sam[0:4000]\n",
    "            silence = AudioSegment.silent(duration=pad_ms)\n",
    "            padded = audio_sam + silence        \n",
    "  \n",
    "        a = padded.get_array_of_samples()\n",
    "        b = np.array(a)\n",
    "        padded_audio.append(b)\n",
    "        \n",
    "        \"\"\" Compute the 64 dimensional filter banks for the 4secs fixed length audio segments\"\"\"\n",
    "    \n",
    "        f_banks = fbank(padded_audio, sample_rate=8000, window_length=400, stft_shift=160, number_of_filters=64,\n",
    "                        stft_size=512,lowest_frequency=0,highest_frequency=None, preemphasis_factor=0.97,\n",
    "                        window=scipy.signal.windows.hamming, denoise=False)\n",
    "        fbank_data.append(f_banks)\n",
    "        float_fbank = np.float32(fbank_data)\n",
    "        float_fbank = np.squeeze(float_fbank,0)\n",
    "        \n",
    "        example['features'] = (float_fbank)\n",
    "    \n",
    "        return example\n",
    "    \n",
    "    def prep_label(example):\n",
    "        #label_dict = pb.io.load_json(path = '/net/home/dheerajpr/my_project/fearless/fearless/sid/labels_argmax.json')\n",
    "        label_hot = pb.io.load_json(path = '/net/home/dheerajpr/my_project/fearless/fearless/sid/labels_167_hot.json')\n",
    "        label_dict3 = pb.io.load_json(path = '/net/home/dheerajpr/my_project/fearless/fearless/sid/labels_167.json')\n",
    "        if example['speaker_id'] in label_dict3.keys():\n",
    "        \n",
    "            pos = label_dict3[example['speaker_id']]\n",
    "            example['label_array'] = np.array(pos)\n",
    "            \n",
    "        if example['speaker_id'] in label_hot.keys():\n",
    "        \n",
    "            pos = label_hot[example['speaker_id']]\n",
    "            example['label_hot'] = np.array(pos)  \n",
    "    \n",
    "        return example\n",
    "        \n",
    "    \n",
    "    def stack(example):\n",
    "        \n",
    "        example['features'] = np.stack(example['features'])\n",
    "        example['label_array'] = np.stack(example['label_array'])\n",
    "        example['label_hot'] = np.stack(example['label_hot'])\n",
    "        \n",
    "        return example\n",
    "    \n",
    "    example = example.map(prep_features)\n",
    "    example = example.map(prep_label)\n",
    "    if shuffle:\n",
    "        example = example.shuffle()\n",
    "   \n",
    "    example = example.batch(batch_size).map(collate_fn)\n",
    "    example = example.map(stack)\n",
    "    #example = example.prefetch(buffer_size=8, num_workers=8)\n",
    "              \n",
    "            \n",
    "            \n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_167 = pb.io.load_json(\"/net/home/dheerajpr/my_project/fearless/fearless/sid/dev_167.json\")\n",
    "train_167 =  pb.io.load_json(\"/net/home/dheerajpr/my_project/fearless/fearless/sid/train_167.json\")\n",
    "dev_sid = db.get_dataset(\"Dev_SID\")\n",
    "train_sid =   db.get_dataset(\"Train_SID\")\n",
    "#dev_50 = pb.io.load_json(\"/net/home/dheerajpr/my_project/fearless/fearless/sid/dev_50.json\")\n",
    "train_167_lazy = lazy_dataset.new(train_167['Train_167'])\n",
    "dev_167_lazy = lazy_dataset.new(dev_167['Dev_167'])\n",
    "#dev_50_lazy = lazy_dataset.new(dev_50['Dev_50'])\n",
    "train_data = prepare_data_4(train_167_lazy,batch_size=1)\n",
    "validation_data = prepare_data_4(dev_167_lazy,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet_SID(\n",
       "  size=ModelParameterSize(total_count=5388634, trainable_count=5388634, total_bytes=21554536, trainable_bytes=21554536)\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): LeakyReLU(negative_slope=0.01)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (avgpool2d): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "  (layer1): Sequential(\n",
       "    (0): Block(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Block(\n",
       "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): LeakyReLU(negative_slope=0.01)\n",
       "      (identity_downsample): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Block(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): LeakyReLU(negative_slope=0.01)\n",
       "      (identity_downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (4): Block(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (5): Block(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Block(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): LeakyReLU(negative_slope=0.01)\n",
       "      (identity_downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       "  (drop_out): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=218, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#exp_dir = '/net/vol/dheerajpr/models/SID/2021-08-03-10-24-31' #167\n",
    "#exp_dir = '/net/vol/dheerajpr/models/SID/2021-08-19-11-13-00' #167 better 3sec\n",
    "#exp_dir = '/net/vol/dheerajpr/models/SID/2021-08-12-13-23-49' #Model strides at 2\n",
    "#exp_dir = '/net/vol/dheerajpr/models/SID/2021-08-12-16-42-54' #LR 0.05, layer 3 and 4 stride 1\n",
    "#exp_dir = '/net/vol/dheerajpr/models/SID/2021-09-04-14-41-51' #167 new 32k data prep validated on train segments\n",
    "\n",
    "#exp_dir = '/net/vol/dheerajpr/models/SID/2021-08-12-21-03-14' #Best result yet(1)\n",
    "#exp_dir = '/net/vol/dheerajpr/models/SID/2021-08-13-10-17-52' #(2)\n",
    "#exp_dir = '/net/vol/dheerajpr/models/SID/2021-08-10-12-43-44' #New var length\n",
    "\n",
    "\n",
    "\n",
    "#exp_dir = '/net/vol/dheerajpr/models/SID/2021-09-05-02-45-38' # 32k proper\n",
    "#exp_dir = '/net/vol/dheerajpr/models/SID/2021-09-05-22-42-44' # Segmented (1)\n",
    "\n",
    "#exp_dir = '/net/vol/dheerajpr/models/SID/2021-09-08-23-13-35' # Segmented (2)\n",
    "#exp_dir = '/net/vol/dheerajpr/models/SID/2021-09-11-12-33-27' # Segmented (3)\n",
    "\n",
    "\n",
    "#exp_dir = '/net/vol/dheerajpr/models/SID/2021-09-28-22-38-53' #100 spk\n",
    "\n",
    "exp_dir = '/net/vol/dheerajpr/models/SID/2021-09-09-20-08-35' #SID dataset\n",
    "#exp_dir = '/net/vol/dheerajpr/models/SID/2021-10-02-10-29-26'  #|SID BIG MODEL\n",
    "\n",
    "#exp_dir = '/net/vol/dheerajpr/models/SID/2021-10-11-18-57-39' # SID shift 160 to 180\n",
    "#exp_dir = '/net/vol/dheerajpr/models/SID/2021-10-12-23-24-11' # SID logfbanks\n",
    "#ckpt_name = 'ckpt_48000.pth'\n",
    "\n",
    "#exp_dir = '/net/vol/dheerajpr/models/SID/2021-10-13-10-35-12' # 167 logfbanks\n",
    "\n",
    "#exp_dir = '/net/vol/dheerajpr/models/SID/2021-10-14-20-49-27' # SID logfbanks 5secs\n",
    "#ckpt_name = 'ckpt_51000.pth'\n",
    "#exp_dir = '/net/vol/dheerajpr/models/SID/2021-10-15-21-17-48'\n",
    "#ckpt_name = 'ckpt_48280.pth'\n",
    "ckpt_name = 'ckpt_best_loss.pth'\n",
    "device = 0\n",
    "model_SID = Model.from_storage_dir(\n",
    "    exp_dir, consider_mpi=True, checkpoint_name=ckpt_name\n",
    ")\n",
    "model_SID.to(device)\n",
    "model_SID.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "del exp_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_data(example,batch_size=1,shuffle=False):\n",
    "    \n",
    "    def prep_features(example):\n",
    "        \n",
    "        fbank_data = []\n",
    "        audio = pb.io.load_audio(example['audio_path']['observation'],dtype=np.int16)\n",
    "        #audio = example['array_samples']\n",
    "        f_banks = pb.transform.fbank(audio, sample_rate=8000, window_length=400, stft_shift=160, number_of_filters=64,\n",
    "                            stft_size=512,lowest_frequency=0,highest_frequency=None, preemphasis_factor=0.97,\n",
    "                            window=scipy.signal.windows.hamming, denoise=False)\n",
    "        fbank_data.append(f_banks)\n",
    "        float_fbank = np.float32(fbank_data)\n",
    "        example['features'] = torch.from_numpy(float_fbank)\n",
    "        \n",
    "        return example\n",
    "    \n",
    "    def prep_label(example):\n",
    "        label_dict = pb.io.load_json(path = '/net/home/dheerajpr/my_project/fearless/fearless/sid/labels_sid.json')\n",
    "        label_dict2 = pb.io.load_json(path = '/net/home/dheerajpr/my_project/fearless/fearless/sid/labels_sid_hot.json')\n",
    "        \n",
    "        if example['speaker_id'] in label_dict.keys():\n",
    "        \n",
    "            pos = label_dict[example['speaker_id']]\n",
    "            \n",
    "            example['label_array'] = np.array(pos) \n",
    "            \n",
    "        if example['speaker_id'] in label_dict2.keys():\n",
    "        \n",
    "            pos = label_dict2[example['speaker_id']]\n",
    "            #pos = np.concatenate((pos,np.zeros(51)))\n",
    "            example['label_hot'] = np.array(pos)\n",
    "        return example\n",
    "        \n",
    "    \n",
    "     \n",
    "    def stack(example):\n",
    "        example['features'] = pad_sequence(example['features'], batch_first=True)\n",
    "        #example['features'] =  torch.unsqueeze(example['features'],1)\n",
    "        example['label_array'] = np.stack(example['label_array'])\n",
    "        example['label_hot'] = np.stack(example['label_hot'])\n",
    "        \n",
    "        return example\n",
    "    \n",
    "    example = example.map(prep_features)\n",
    "    example = example.map(prep_label)    \n",
    "    \n",
    "    if shuffle:\n",
    "        example = example.shuffle()\n",
    "    example = example.batch(batch_size).map(collate_fn)\n",
    "    example = example.map(stack)          \n",
    "            \n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_100 = pb.io.load_json(\"/net/home/dheerajpr/my_project/fearless/fearless/sid/labels_100.json\")\n",
    "labels_50 = pb.io.load_json(\"/net/home/dheerajpr/my_project/fearless/fearless/sid/labels_50.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dev_sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_100 = df[df['speaker_id'].isin(labels_100.keys())]\n",
    "list_100 = pd.DataFrame.to_dict(df_100,'records')\n",
    "df_50 = df[df['speaker_id'].isin(labels_50.keys())]\n",
    "list_50 = pd.DataFrame.to_dict(df_50,'records')\n",
    "ds_100 = lazy_dataset.new(list_100)\n",
    "ds_50 = lazy_dataset.new(list_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            DictDataset(name='Dev_SID', len=6373)\n",
       "          MapDataset(_pickle.loads)\n",
       "        MapDataset(<function evaluation_data.<locals>.prep_features at 0x7f38fe5a6680>)\n",
       "      MapDataset(<function evaluation_data.<locals>.prep_label at 0x7f38fe5a6710>)\n",
       "    BatchDataset(batch_size=1)\n",
       "  MapDataset(<function collate_fn at 0x7f3a19a49710>)\n",
       "MapDataset(<function evaluation_data.<locals>.stack at 0x7f38fe5a6e60>)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#eval_dev_sid = evaluation_data(dev_sid)\n",
    "eval_dev = evaluation_data(dev_sid,batch_size=1)\n",
    "eval_train =  evaluation_data(train_167_lazy)\n",
    "eval_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio_path': {'observation': ['/net/db/fearless/Audio/Segments/SID/Dev/FS02_SID_dev_0001.wav']},\n",
       " 'num_samples': [25760],\n",
       " 'speaker_id': ['NETWORK5'],\n",
       " 'example_id': ['FS02_SID_dev_0001'],\n",
       " 'dataset': ['Dev_SID'],\n",
       " 'features': tensor([[[[ 1.9220,  7.3107,  8.6591,  ...,  6.0031,  6.5285,  5.1012],\n",
       "           [ 2.3762,  7.5539,  8.8342,  ...,  5.4482,  5.6939,  4.3270],\n",
       "           [ 2.4039,  7.7061,  9.0035,  ...,  6.5840,  6.2537,  4.1797],\n",
       "           ...,\n",
       "           [ 2.3579,  7.9967, 11.2510,  ...,  7.6528,  6.7941,  5.7264],\n",
       "           [ 4.7742,  6.3954,  9.8499,  ...,  5.2437,  5.6866,  4.3517],\n",
       "           [ 1.3189,  6.9546,  8.0696,  ...,  5.2239,  5.3703,  3.6732]]]]),\n",
       " 'label_array': array([142]),\n",
       " 'label_hot': array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dev[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation for the train segment dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6373/6373 [02:19<00:00, 45.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 0.6635807312097913,\n",
      " 'F1_score': 0.6635807312097913,\n",
      " 'Precision': 0.6635807312097913,\n",
      " 'Recall': 0.6635807312097913,\n",
      " 'Top_1_Accuracy': 0.6635807,\n",
      " 'Top_2_Accuracy': 0.7693394,\n",
      " 'Top_5_Accuracy': 0.86552644}\n"
     ]
    }
   ],
   "source": [
    "#Evaluate\n",
    "\n",
    "with torch.no_grad():\n",
    "        metric = {'Accuracy':[],'Top_5_Accuracy':[]}\n",
    "        Accuracy = []\n",
    "        Precision = []\n",
    "        Recall = []\n",
    "        F1 = []\n",
    "        Top_5 = []\n",
    "        Top_2 = []\n",
    "        Top_1 = []\n",
    "        scores = []\n",
    "        targets = []\n",
    "        fbank_data = []\n",
    "        \n",
    "        for example in tqdm(eval_dev):          \n",
    "            \n",
    "            example = model_SID.example_to_device(example, device)\n",
    "            output = model_SID(example)            \n",
    "            pred = output['prediction'].cpu().detach().numpy()\n",
    "            prediction_soft = softmax(pred)\n",
    "            prediction_max = np.argmax(pred, axis=-1)\n",
    "            target = example['label_array'].cpu().detach().numpy()\n",
    "            target_hot = example['label_hot'].cpu().detach().numpy()\n",
    "            #print(target_hot)\n",
    "            accuracy = (prediction_max == target).mean()            \n",
    "           \n",
    "            Accuracy.append(accuracy)  \n",
    "            \n",
    "            top_5 = tf.keras.metrics.sparse_top_k_categorical_accuracy(target,prediction_soft, k=5)\n",
    "            top_5 = np.array(top_5).mean()\n",
    "            Top_5.append(top_5)\n",
    "            \n",
    "            top_2 = tf.keras.metrics.sparse_top_k_categorical_accuracy(target,prediction_soft, k=2)\n",
    "            top_2 = np.array(top_2).mean()\n",
    "            Top_2.append(top_2)\n",
    "            \n",
    "            top_1 = tf.keras.metrics.sparse_top_k_categorical_accuracy(target,prediction_soft, k=1)\n",
    "            top_1 = np.array(top_1).mean()\n",
    "            Top_1.append(top_1)\n",
    "            \n",
    "            precision = metrics.precision_score(target,prediction_max, average='macro', zero_division=0)\n",
    "            Precision.append(precision)\n",
    "            recall = metrics.recall_score(target,prediction_max, average='macro', zero_division=0)\n",
    "            Recall.append(recall)\n",
    "            f1 = metrics.f1_score(target,prediction_max, average='macro', zero_division=0)\n",
    "            F1.append(f1)\n",
    "            \n",
    "            targets.append(target_hot)            \n",
    "            scores.append(prediction_soft)\n",
    "            \n",
    "        targets_con = np.concatenate((targets))\n",
    "        scores_con = np.concatenate((scores))\n",
    "        #thr,met = sed.get_optimal_thresholds(targets_con,scores_con,metric='f1')       \n",
    "        #decisions = scores_con > thr\n",
    "        #print(decisions)\n",
    "        #f1, p, r = sed.fscore(targets, decisions, event_wise=False)\n",
    "       # auc = metrics.roc_auc_score(targets_con, scores_con, None)\n",
    "                    \n",
    "        metric['Accuracy'] = np.mean(Accuracy)\n",
    "        metric['Top_5_Accuracy'] = np.mean(Top_5)\n",
    "        metric['Top_2_Accuracy'] = np.mean(Top_2)\n",
    "        metric['Top_1_Accuracy'] = np.mean(Top_1)\n",
    "        metric['Precision'] = np.mean(Precision)\n",
    "        metric['Recall'] = np.mean(Recall)\n",
    "        metric['F1_score'] = np.mean(F1)\n",
    "        #metric['AUC'] = np.mean(auc)\n",
    "        #metric['Thresholds'] = thr\n",
    "        \n",
    "        pprint(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Accuracy': 0.768868664679115,\n",
       " 'Top_5_Accuracy': 0.9165228,\n",
       " 'Top_2_Accuracy': 0.85218894,\n",
       " 'Top_1_Accuracy': 0.7688687,\n",
       " 'Precision': 0.768868664679115,\n",
       " 'Recall': 0.768868664679115,\n",
       " 'F1_score': 0.768868664679115}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6373, 218), (6373, 218))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_con.shape, scores_con.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'thr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-1e0f159ff33e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdecisions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores_con\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mthr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'thr' is not defined"
     ]
    }
   ],
   "source": [
    "decisions = scores_con>thr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'decisions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-28b01510ec51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets_con\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecisions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_wise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'decisions' is not defined"
     ]
    }
   ],
   "source": [
    "f1, p, r = sed.fscore(targets_con, decisions,beta=1.0, event_wise=False)\n",
    "f1,p,r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_argmax = []\n",
    "scores_argmax = []\n",
    "for i in targets_con:\n",
    "    targets_argmax.append(np.argmax(i))\n",
    "\n",
    "for i in scores_con:\n",
    "    scores_argmax.append(np.argmax(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.652318935993544 : weighted\n",
      "0.3467637752558192 : macro\n",
      "0.6635807312097913 : micro\n"
     ]
    }
   ],
   "source": [
    "print(metrics.f1_score(targets_argmax,scores_argmax,average='weighted'), ':', 'weighted')\n",
    "print(metrics.f1_score(targets_argmax,scores_argmax,average='macro',zero_division=0), ':', 'macro')\n",
    "print(metrics.f1_score(targets_argmax,scores_argmax,average='micro'), ':', 'micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = metrics.confusion_matrix(targets_argmax,scores_argmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/software/python/2020_06/anaconda/lib/python3.7/site-packages/ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n",
      "/net/software/python/2020_06/anaconda/lib/python3.7/site-packages/ipykernel_launcher.py:22: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix) \n",
    "FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "TP = np.diag(cnf_matrix)\n",
    "TN = cnf_matrix.sum() - (FP + FN + TP)\n",
    "FP = FP.astype(float)\n",
    "FN = FN.astype(float)\n",
    "TP = TP.astype(float)\n",
    "TN = TN.astype(float)\n",
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "# Precision or positive predictive value\n",
    "PPV = TP/(TP+FP)\n",
    "# Negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "# False discovery rate\n",
    "FDR = FP/(TP+FP)\n",
    "# Overall accuracy for each class\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "#DCF for each class\n",
    "DCF = (0.75*FNR)+(0.25*FPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.00011775788977861517, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(DCF), np.argmin(DCF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = []\n",
    "for i in dev_sid:\n",
    "    num_samples.append(i['num_samples'])\n",
    "\n",
    "num_samples = [x / 8000 for x in num_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>targets</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.22</td>\n",
       "      <td>142</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.18</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.47</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.22</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.47</td>\n",
       "      <td>73</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6368</th>\n",
       "      <td>3.31</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6369</th>\n",
       "      <td>4.23</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6370</th>\n",
       "      <td>4.14</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6371</th>\n",
       "      <td>4.45</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6372</th>\n",
       "      <td>3.00</td>\n",
       "      <td>138</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6373 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      duration  targets  scores\n",
       "0         3.22      142     196\n",
       "1         4.18       19      19\n",
       "2         3.47       70      70\n",
       "3         4.22       73      73\n",
       "4         4.47       73     167\n",
       "...        ...      ...     ...\n",
       "6368      3.31      201     201\n",
       "6369      4.23       19      19\n",
       "6370      4.14       19      19\n",
       "6371      4.45       19      19\n",
       "6372      3.00      138     138\n",
       "\n",
       "[6373 rows x 3 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dict = {}\n",
    "new_dict['duration'] = num_samples\n",
    "new_dict['targets'] = targets_argmax\n",
    "new_dict['scores'] = scores_argmax\n",
    "\n",
    "pd = pandas.DataFrame.from_dict(new_dict)\n",
    "pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_column = np.where(pd['targets']==pd['scores'],True, False)\n",
    "pd[\"match\"] = comparison_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = pd[pd[\"match\"]==True]\n",
    "\n",
    "pd_sort_dur = pd.sort_values(by=['duration'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dur_acc(dur):\n",
    "    dur_res =  pd_sort_dur.loc[(pd[\"duration\"]>=dur)]        \n",
    "    return len(dur_res[dur_res[\"match\"]==True])/len(dur_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-4b32fec22099>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdur_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-116-04a654ac0cca>\u001b[0m in \u001b[0;36mdur_acc\u001b[0;34m(dur)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdur_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdur\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdur_res\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mpd_sort_dur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"duration\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0mdur\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdur_res\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdur_res\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"match\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdur_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "y = []\n",
    "for i in np.arange(0,20,0.5):\n",
    "    y.append(dur_acc(i))\n",
    "    \n",
    "x = np.arange(0,20,0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-f69e67cd96d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Duration (s)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Duration vs Accuracies for Train Segments'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(x,y)\n",
    "plt.xlabel(\"Duration (s)\"),plt.ylabel('Accuracy'),plt.title('Duration vs Accuracies for Train Segments')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground-truth speaker and count\n",
    "\n",
    "unique, counts = np.unique(targets_argmax, return_counts=True)\n",
    "dict1 = dict(zip(unique, counts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = [i for i, j in zip(new_dict['targets'],new_dict['scores']) if i == j]  #Correctly classified samples based on speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correctly classified speaker and count\n",
    "\n",
    "unique_1, counts_1 = np.unique(v, return_counts=True)\n",
    "dict2 = dict(zip(unique_1, counts_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in dict1.items():\n",
    "    print(key, ' : ', value)  #Ground truth speakers and their segment count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in dict2.items():\n",
    "    print(key, ' : ', value)   #Correctly classified speakers and their segment count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracies for each class/speaker.\n",
    "import operator\n",
    "\n",
    "accs = []\n",
    "for i in dict1.keys():\n",
    "    if i in dict2.keys():\n",
    "        accs.append(dict2[i]/dict1[i])\n",
    "    \n",
    "d = dict(zip(unique_1,accs))\n",
    "#cd = sorted(d.items(),key=operator.itemgetter(1),reverse=True)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean of the accuracies\n",
    "mean(accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation for the validation segment dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fearless.sid.segmenter import segmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_167_lazy[8997]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data(example,segment_size=32000):\n",
    "    \n",
    "    to_chunk = example\n",
    "    segmented = segmenter(to_chunk,segment_size)\n",
    "    segmented = lazy_dataset.new(segmented)\n",
    "    eval_segmented = evaluation_data(segmented)\n",
    "    \n",
    "    return eval_segmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval_data = chunk_data(dev_167_lazy[8997:8998],segment_size=int(4000*3.21)) #buzz_longest in dev segment ds (3.21)\n",
    "eval_data = chunk_data(dev_167_lazy[8:9],segment_size=int(4000*8.385))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval1 = []\n",
    "for i in np.arange(0.5,20,0.5):\n",
    "    eval1.append(chunk_data(dev_167_lazy[8:9],segment_size=int(4000*i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Evaluate\n",
    "\n",
    "with torch.no_grad():\n",
    "        metric = {'Accuracy':[],'Top_5_Accuracy':[]}\n",
    "        Accuracy = []\n",
    "        Precision = []\n",
    "        Recall = []\n",
    "        F1 = []\n",
    "        Top_5 = []\n",
    "        scores = []\n",
    "        targets = []\n",
    "        fbank_data = []\n",
    "        \n",
    "        for example in tqdm(eval_data):\n",
    "                \n",
    "                #example = eval_dev[8]\n",
    "                example = model_SID.example_to_device(example, device)\n",
    "                output = model_SID(example)            \n",
    "                pred = output['prediction'].cpu().detach().numpy()\n",
    "                prediction_soft = softmax(pred)\n",
    "                prediction_max = np.argmax(pred, axis=-1)\n",
    "                target = example['label_array'].cpu().detach().numpy()\n",
    "                target_hot = example['label_hot'].cpu().detach().numpy()\n",
    "                #print(np.argmax(prediction_soft))\n",
    "                accuracy = (prediction_max == target).mean()            \n",
    "                #print(accuracy)\n",
    "                Accuracy.append(accuracy)  \n",
    "                top_5 = tf.keras.metrics.sparse_top_k_categorical_accuracy(target,prediction_soft, k=5)\n",
    "                top_5 = np.array(top_5).mean()\n",
    "                Top_5.append(top_5)\n",
    "                \n",
    "                precision = metrics.precision_score(target,prediction_max, average='weighted', zero_division=0)\n",
    "                Precision.append(precision)\n",
    "                recall = metrics.recall_score(target,prediction_max, average='weighted', zero_division=0)\n",
    "                Recall.append(recall)\n",
    "                f1 = metrics.f1_score(target,prediction_max, average='weighted', zero_division=0)\n",
    "                F1.append(f1)\n",
    "                \n",
    "                targets.append(target_hot)            \n",
    "                scores.append(prediction_soft)\n",
    "                \n",
    "        targets_con = np.concatenate((targets))\n",
    "        scores_con = np.concatenate((scores))\n",
    "        #thr,met = sed.get_optimal_thresholds(targets_con,scores_con,metric='f1')       \n",
    "        #decisions = scores_con > thr\n",
    "        #print(decisions)\n",
    "        #f1, p, r = sed.fscore(targets, decisions, event_wise=False)\n",
    "        #auc = metrics.roc_auc_score(targets_con, scores_con, None)\n",
    "                    \n",
    "        metric['Accuracy'] = np.mean(Accuracy)\n",
    "        metric['Top_5_Accuracy'] = np.mean(Top_5)\n",
    "        metric['Precision'] = np.mean(Precision)\n",
    "        metric['Recall'] = np.mean(Recall)\n",
    "        metric['F1_score'] = np.mean(F1)\n",
    "        #metric['AUC'] = np.mean(auc)\n",
    "        #metric['Thresholds'] = thr\n",
    "        \n",
    "        pprint(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_con.shape, scores_con.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisions = scores_con>thr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1, p, r = sed.fscore(targets_con, decisions,beta=1.0, event_wise=False)\n",
    "f1,p,r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_argmax = []\n",
    "scores_argmax = []\n",
    "for i in targets_con:\n",
    "    targets_argmax.append(np.argmax(i))\n",
    "\n",
    "for i in scores_con:\n",
    "    scores_argmax.append(np.argmax(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.f1_score(targets_argmax,scores_argmax,average='weighted'), ':', 'weighted')\n",
    "print(metrics.f1_score(targets_argmax,scores_argmax,average='macro'), ':', 'macro')\n",
    "print(metrics.f1_score(targets_argmax,scores_argmax,average='micro'), ':', 'micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = metrics.confusion_matrix(targets_argmax,scores_argmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix) \n",
    "FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "TP = np.diag(cnf_matrix)\n",
    "TN = cnf_matrix.sum() - (FP + FN + TP)\n",
    "FP = FP.astype(float)\n",
    "FN = FN.astype(float)\n",
    "TP = TP.astype(float)\n",
    "TN = TN.astype(float)\n",
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "# Precision or positive predictive value\n",
    "PPV = TP/(TP+FP)\n",
    "# Negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "# False discovery rate\n",
    "FDR = FP/(TP+FP)\n",
    "# Overall accuracy for each class\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "#DCF for each class\n",
    "DCF = (0.75*FNR)+(0.25*FPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = []\n",
    "for i in dev_167_lazy:\n",
    "    num_samples.append(i['num_samples'])\n",
    "\n",
    "num_samples = [x / 8000 for x in num_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = pandas.DataFrame(dev_167_lazy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict = {}\n",
    "new_dict['audio_file'] = dev_df['example_id']\n",
    "new_dict['duration'] = num_samples\n",
    "new_dict['targets'] = targets_argmax\n",
    "new_dict['scores'] = scores_argmax\n",
    "new_dict\n",
    "df = pandas.DataFrame.from_dict(new_dict)\n",
    "df1 = df[df['targets']==df['scores']]\n",
    "\n",
    "buzz_df = df1[df1['targets']==12]\n",
    "\n",
    "buzz_dict = pandas.DataFrame.from_dict(buzz_df)\n",
    "\n",
    "buzz_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_column = np.where(pd['targets']==pd['scores'],True, False)\n",
    "pd[\"match\"] = comparison_column\n",
    "\n",
    "duration = pd[pd[\"match\"]==True]\n",
    "\n",
    "pd_sort_dur = pd.sort_values(by=['duration'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dur_acc(dur):\n",
    "    dur_res =  pd_sort_dur.loc[(pd[\"duration\"]>=dur)]        \n",
    "    return len(dur_res[dur_res[\"match\"]==True])/len(dur_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "for i in np.arange(0.5,20,0.5):\n",
    "    y.append(dur_acc(i))\n",
    "\n",
    "x = np.arange(0.5,20,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y)\n",
    "plt.xlabel(\"Duration (s)\"),plt.ylabel('Accuracy'),plt.title('Duration vs Accuracies for Dev Segments')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground-truth speaker and count\n",
    "\n",
    "unique, counts = np.unique(targets_argmax, return_counts=True)\n",
    "dict1 = dict(zip(unique, counts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = [i for i, j in zip(new_dict['targets'],new_dict['scores']) if i == j]  #Correctly classified samples based on speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correctly classified speaker and count\n",
    "\n",
    "unique_1, counts_1 = np.unique(v, return_counts=True)\n",
    "dict2 = dict(zip(unique_1, counts_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.balanced_accuracy_score(targets_argmax,scores_argmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in dict1.items():\n",
    "    print(key, ' : ', value)  #Ground truth speakers and their segment count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in dict2.items():\n",
    "    print(key, ' : ', value)   #Correctly classified speakers and their segment count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracies for each class/speaker.\n",
    "import operator\n",
    "\n",
    "accs = []\n",
    "for i in dict1.keys():\n",
    "    if i in dict2.keys():\n",
    "        accs.append(dict2[i]/dict1[i])\n",
    "    \n",
    "d = dict(zip(unique_1,accs))\n",
    "#cd = sorted(d.items(),key=operator.itemgetter(1),reverse=True)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean of the accuracies\n",
    "mean(accs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_2020",
   "language": "python",
   "name": "project_2020"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
