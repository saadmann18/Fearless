{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These magics make working in the notebook a little easier\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import paderbox as pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is where default works\n"
     ]
    }
   ],
   "source": [
    "# Import ASR Task - This is the only thing needed to start the whole training\n",
    "from espnet2.tasks.asr import ASRTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(accum_grad=8, allow_variable_data_keys=True, batch_bins=1000000, batch_size=6, batch_type='folded', best_model_criterion=[['valid', 'acc', 'max']], bpemodel=None, chunk_length=500, chunk_shift_ratio=0.5, cleaner=None, collect_stats=False, config='conf/train_asr_transformer.yaml', ctc_conf={'dropout_rate': 0.0, 'ctc_type': 'builtin', 'reduce': True, 'ignore_nan_grad': False}, cudnn_benchmark=False, cudnn_deterministic=True, cudnn_enabled=True, decoder='transformer', decoder_conf={'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 6, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'self_attention_dropout_rate': 0.0, 'src_attention_dropout_rate': 0.0}, dist_backend='nccl', dist_init_method='env://', dist_launcher=None, dist_master_addr=None, dist_master_port=None, dist_rank=None, dist_world_size=None, distributed=False, dry_run=False, early_stopping_criterion=['valid', 'loss', 'min'], encoder='transformer', encoder_conf={'output_size': 256, 'attention_heads': 4, 'linear_units': 2048, 'num_blocks': 12, 'dropout_rate': 0.1, 'positional_dropout_rate': 0.1, 'attention_dropout_rate': 0.0, 'input_layer': 'conv2d', 'normalize_before': True}, fold_length=[80000, 150], frontend='default', frontend_conf={'fs': '16k', 'hop_length': 60}, g2p=None, grad_clip=5.0, grad_clip_type=2.0, grad_noise=False, init='xavier_uniform', init_param=[], input_size=None, iterator_type='sequence', keep_nbest_models=10, local_rank=0, log_interval=None, log_level='INFO', max_cache_fd=32, max_cache_size=0.0, max_epoch=150, model_conf={'ctc_weight': 0.3, 'lsm_weight': 0.1, 'length_normalized_loss': False}, multiple_iterator=False, multiprocessing_distributed=False, ngpu=1, no_forward_run=False, non_linguistic_symbols='data/nlsyms.txt', normalize='global_mvn', normalize_conf={'stats_file': '/net/vol/vivekkan/experiments/fearless/asr1/exp/asr_stats_raw_char/train/feats_stats.npz'}, num_att_plot=3, num_cache_chunks=1024, num_iters_per_epoch=None, num_workers=1, optim='adam', optim_conf={'lr': 0.005}, output_dir='/net/vol/vivekkan/experiments/fearless/asr1/exp/asr_SpkVec', patience=None, pretrain_path=None, print_config=False, required=['output_dir', 'token_list'], resume=True, scheduler='warmuplr', scheduler_conf={'warmup_steps': 30000}, seed=0, sort_batch='descending', sort_in_batch='descending', specaug=None, specaug_conf={}, token_list=['<blank>', '<unk>', '<space>', '<VOCALIZATION>', 'E', 'T', 'A', 'N', 'I', 'O', 'S', 'R', 'H', 'L', 'D', 'C', 'U', 'M', 'P', 'F', 'G', 'Y', 'W', 'B', 'V', 'K', '.', 'X', \"''\", 'J', 'Q', 'Z', ',', '-', '\"', '*', ':', '(', ')', '?', '!', '&', ';', '1', '2', '0', '/', '$', '{', '}', '8', '9', '6', '3', '5', '7', '4', '~', '`', '_', '<*IN*>', '<*MR.*>', '\\\\', '^', '<sos/eos>'], token_type='char', train_data_path_and_name_and_type=[['/net/vol/vivekkan/experiments/fearless/asr1/dump/raw/trainset/wav.scp', 'speech', 'sound'], ['/net/vol/vivekkan/experiments/fearless/asr1/dump/raw/trainset/text', 'text', 'text'], ['/net/vol/vivekkan/experiments/fearless/asr1/dump/raw/trainset/utt2spkvec.scp', 'speaker_id', 'npy']], train_dtype='float32', train_shape_file=['/net/vol/vivekkan/experiments/fearless/asr1/exp/asr_stats_raw_char/train/speech_shape', '/net/vol/vivekkan/experiments/fearless/asr1/exp/asr_stats_raw_char/train/text_shape.char'], unused_parameters=False, use_amp=False, use_preprocessor=True, use_tensorboard=True, use_wandb=False, val_scheduler_criterion=['valid', 'loss'], valid_batch_bins=None, valid_batch_size=None, valid_batch_type=None, valid_data_path_and_name_and_type=[['/net/vol/vivekkan/experiments/fearless/asr1/dump/raw/devset/wav.scp', 'speech', 'sound'], ['/net/vol/vivekkan/experiments/fearless/asr1/dump/raw/devset/text', 'text', 'text'], ['/net/vol/vivekkan/experiments/fearless/asr1/dump/raw/devset/utt2spkvec.scp', 'speaker_id', 'npy']], valid_max_cache_size=None, valid_shape_file=['/net/vol/vivekkan/experiments/fearless/asr1/exp/asr_stats_raw_char/valid/speech_shape', '/net/vol/vivekkan/experiments/fearless/asr1/exp/asr_stats_raw_char/valid/text_shape.char'], wandb_id=None, wandb_project=None, write_collected_feats=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a little bit hacky: ESPnet uses the argparser for everything (terrible idea in my opinion), \n",
    "# so we have to fake the command line arguments by directly writing the config into a `argparse.Namespace`.\n",
    "# You can write the config by yourself or read in a config file of a finished training. It is in \".../exp/asr_train.../config.yaml\"\n",
    "import argparse\n",
    "\n",
    "# TODO: insert path to config file\n",
    "#config_file = '/net/vol/vivekkan/experiments/fearless/asr1/exp/asr_train_asr_transformer_raw_char/config.yaml'\n",
    "config_file = '/net/vol/vivekkan/experiments/fearless/asr1/exp/asr_SpkVec/config.yaml'\n",
    "config = pb.io.load(config_file)\n",
    "\n",
    "args = argparse.Namespace(**config)\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = ASRTask.build_model(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another hacky thing: You have to be in the experiment directory for the ASRTask to find the data files, so change directory there\n",
    "import os\n",
    "# TODO: insert path to fearless directory in your home\n",
    "os.chdir('/net/home/vivekkan/my_project/espnet/egs2/fearless/asr1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the iterator that loads the data. Or what ESPnet does: Build a factory that returns an iterator for that\n",
    "from espnet2.train.distributed_utils import DistributedOption\n",
    "from espnet2.utils.build_dataclass import build_dataclass\n",
    "\n",
    "# Set mode='valid' for validation data\n",
    "iter_factory = ASRTask.build_iter_factory(args, distributed_option=build_dataclass(DistributedOption, args), mode='train')\n",
    "iterator = iter_factory.build_iter(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method ASRTask.build_model of <class 'espnet2.tasks.asr.ASRTask'>>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ASRTask.build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one batch from the iterator, in the format that ESPnet requires it\n",
    "it = iter(iterator)\n",
    "batch = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['FIDO1-FS02_ASR_track2_train_08524',\n",
       "  'FD1-FS02_ASR_track2_train_34078',\n",
       "  'FD1-FS02_ASR_track2_train_26235',\n",
       "  'FD1-FS02_ASR_track2_train_24148',\n",
       "  'FD1-FS02_ASR_track2_train_14923',\n",
       "  'FAO2-FS02_ASR_track2_train_20762'],\n",
       " dict_keys(['speech', 'speech_lengths', 'text', 'text_lengths', 'speaker_id', 'speaker_id_lengths']))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch consists of the example IDs and the data in the format specified in the args (train_data_path_and_name_and_type config value(s))\n",
    "# The keys in data should correspond to the arguments of ESPnetASRModel.forward\n",
    "example_ids, data = batch\n",
    "example_ids, data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([52800])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['speech'][3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass one example through the model. The output countains the loss and some statistics to report to tensorboard.\n",
    "# If you don't want to compute the backward step in the notebook, it is better to use torch.no_grad to save some memory and speed things up\n",
    "with torch.no_grad():\n",
    "    model_out = model(**data)\n",
    "loss, stats, weight = model_out\n",
    "loss, stats, weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass one example through the model. The output countains the loss and some statistics to report to tensorboard.\n",
    "# If you don't want to compute the backward step in the notebook, it is better to use torch.no_grad to save some memory and speed things up\n",
    "with torch.no_grad():\n",
    "    model_out = model(**data)\n",
    "loss, stats, weight, featsU ,featsULen = model_out\n",
    "loss, stats, weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['speech'].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech = data['speech'].unsqueeze(0).to(getattr(torch, 'float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = speech.new_full([1], dtype=torch.long, fill_value=speech.size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featsU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats1[0][:][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featsU.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featsU[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['speaker_id'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX= torch.Tensor\n",
    "for i in range(len(featsU)):\n",
    "    for j in range(len(featsU[i])):\n",
    "        testX[i,j,:]= torch.cat((featsU[i][j],data['speaker_id'][i]), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['speaker_id'][0].reshape(3,83)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featsU[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(featsU[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['speaker_id'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testSpeak=tf.expand_dims(data['speaker_id'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Important\n",
    "K=featsU.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Important\n",
    "SpkFeat= data['speaker_id'].unsqueeze(1).repeat(1, K, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SpkFeat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Important\n",
    "SpkFeat=torch.cat((SpkFeat,featsU),dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SpkFeat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['speaker_id_lengths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featsU.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestUn.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_2020",
   "language": "python",
   "name": "project_2020"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
